{/*
I can include a comment here that wont be sent to the LLM... USEFUL!
The output of every LLM call laid out like this will be sent in <think> tags like a reasoning model
except for the final LLM call, which will have the output as the response
*/}

<llm model="deepseek-r1">
    <prompt>
        You are a helpful assistant that can answer questions and help with tasks.
        Think step by step.
    </prompt>
</llm>

<llm model="qwen-2.5-32b">
    <instructions>
        Answer the user's input of ${userInput.message}
        {({lastElement}) => `You had been thinking this... ${lastElement.output}`}
        Now, 
    </instructions>
    <prompt>
        You are a helpful assistant that can answer questions and help with tasks.
    </prompt>
</llm>